{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import random\n",
    "import csv\n",
    "import sqlite3 as lite\n",
    "import unicodedata\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "\n",
    "chunk_size = 1000000\n",
    "tsv_path = \"../res/data.tsv\"\n",
    "# the columns in the tsv source\n",
    "header_names = [\"q_id\",\"question\",\"answer\",\"label\",\"a_id\"]\n",
    "\n",
    "#Will be created if doesnot exists\n",
    "db_path  = \"data.db\"\n",
    "table_name = \"dataset\"\n",
    "\n",
    "con = None\n",
    "\n",
    "try:\n",
    "\n",
    "    file_read = 0\n",
    "    con = lite.connect(db_path)\n",
    "\n",
    "    for chunk in pd.read_csv(tsv_path,sep='\\t',chunksize=chunk_size, header=None, names=header_names, encoding='utf8'):\n",
    "        file_read+=chunk.shape[0]\n",
    "        print(file_read, \"\\t Done\")\n",
    "        chunk.to_sql(name=table_name, con=con, if_exists='append', index=False)\n",
    "\n",
    "except lite.Error:\n",
    "    print(\"SQLite Error\")\n",
    "    sys.exit(1)\n",
    "\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.clock() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BM25 is only a point to start with. \n",
    "Ideally the samples should be selected in a more sophisticated manner.\n",
    "'''\n",
    "#print(os.path.basename(__file__), \"\\n\\n\")\n",
    "start_time = time.clock()\n",
    "\n",
    "docIDFDict = {}\n",
    "avgDocLength = 0\n",
    "docIDFDict_file    = \"../res/docIDFDict.pickle\"\n",
    "avgDocLength_file  = \"../res/avgDocLength.pickle\"\n",
    "\n",
    "fileObject = open(docIDFDict_file, 'rb')\n",
    "docIDFDict = pickle.load(fileObject)\n",
    "fileObject.close()\n",
    "\n",
    "fileObject = open(avgDocLength_file, 'rb')\n",
    "avgDocLength = pickle.load(fileObject)\n",
    "fileObject.close()\n",
    "\n",
    "def GetBM25Score(Query, Passage, k1=1.5, b=0.75, delimiter=' ') :\n",
    "    global docIDFDict,avgDocLength\n",
    "\n",
    "    #remove special characters from query and passage\n",
    "    Query = re.sub(r\"[^a-zA-Z0-9]+\", ' ', Query )\n",
    "    Passage = re.sub(r\"[^a-zA-Z0-9]+\", ' ', Passage)\n",
    "\n",
    "    query_words= Query.strip().lower().split(delimiter)\n",
    "    passage_words = Passage.strip().lower().split(delimiter)\n",
    "    passageLen = len(passage_words)\n",
    "    docTF = {}\n",
    "    for word in set(query_words):   #Find Term Frequency of all query unique words\n",
    "        docTF[word] = passage_words.count(word)\n",
    "    commonWords = set(query_words) & set(passage_words)\n",
    "    tmp_score = []\n",
    "    for word in commonWords :\n",
    "        numer = (docTF[word] * (k1+1))   #Numerator part of BM25 Formula\n",
    "        denom = ((docTF[word]) + k1*(1 - b + b*passageLen/avgDocLength)) #Denominator part of BM25 Formula\n",
    "        if(word in docIDFDict) :\n",
    "            tmp_score.append(docIDFDict[word] * numer / denom)\n",
    "    score = sum(tmp_score)\n",
    "    return score\n",
    "\n",
    "\n",
    "print(\"BM25 Initialized\")\n",
    "\n",
    "chunk_size = 1000000\n",
    "header_names = [\"q_id\",\"question\",\"answer\",\"label\",\"a_id\"]\n",
    "bm25_header_names = [\"q_id\",\"question\",\"answer\",\"label\",\"a_id\",\"bm25_score\"]\n",
    "#Will be created if doesnot exists\n",
    "db_path  = \"data.db\"\n",
    "table_name = \"dataset\"\n",
    "bm25_table_name = \"dataset_bm25\"\n",
    "\n",
    "sql_read_query = \"SELECT * FROM \"+table_name\n",
    "\n",
    "con = None\n",
    "try:\n",
    "    file_read = 0\n",
    "    con = lite.connect(db_path)\n",
    "    print(\"SQLite connection establshed\")\n",
    "\n",
    "    for chunk in pd.read_sql_query(sql=sql_read_query, con=con,chunksize=chunk_size):\n",
    "        chunk = chunk.reindex(columns=bm25_header_names)\n",
    "        chunk[\"bm25_score\"] = chunk.apply(lambda x: GetBM25Score(x.question, x.answer), axis=1)\n",
    "        chunk.to_sql(name=bm25_table_name, con=con, if_exists='append', index=False)\n",
    "        file_read += chunk.shape[0]\n",
    "        print(file_read, \"\\t Done\")\n",
    "\n",
    "    cursor = con.cursor()\n",
    "    cursor.execute('''DROP TABLE dataset''')\n",
    "    con.commit()\n",
    "\n",
    "except lite.Error as e:\n",
    "    print(\"SQLite Error\")\n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.clock() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.clock()\n",
    "curr_time = time.clock()\n",
    "\n",
    "chunk_size = 100000 #because 10 answers for a question\n",
    "bm25_header_names = [\"q_id\",\"question\",\"answer\",\"label\",\"a_id\",\"bm25_score\"]\n",
    "db_path  = \"data.db\"\n",
    "rearranged_table_name = \"dataset_rearranged\"\n",
    "table_name = \"dataset_bm25\"\n",
    "\n",
    "total_rows = 5241880\n",
    "\n",
    "# sql_read_query = \"SELECT * FROM \"+ rearranged_table_name # +\" ORDER BY bm25_score\"\n",
    "sql_read_query = \"SELECT * FROM \"+ table_name +\" ORDER BY question , bm25_score\"\n",
    "sql_drop_query = \"DROP TABLE \" + table_name\n",
    "\n",
    "\n",
    "con = None\n",
    "try:\n",
    "    skip_count = 0\n",
    "    file_read = 0\n",
    "    counter = 0\n",
    "    con = lite.connect(db_path)\n",
    "    print(\"SQLite connection established\")\n",
    "\n",
    "    for chunk in pd.read_sql_query(sql=sql_read_query, con=con,chunksize=chunk_size):\n",
    "        file_read += chunk.shape[0]\n",
    "        chunk.to_sql(name=rearranged_table_name, con=con, if_exists='append', index=False)\n",
    "        time_taken = time.clock() - curr_time\n",
    "        print(file_read,\" Done\\t Time taken : \",(time_taken), \"\\t ETA : \", (((total_rows-file_read)/chunk_size)*time_taken))\n",
    "        curr_time = time.clock()\n",
    "\n",
    "    cursor = con.cursor()\n",
    "    cursor.execute(sql_drop_query)\n",
    "    con.commit()\n",
    "\n",
    "except lite.Error as e:\n",
    "    print(\"SQLite Error\")\n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.clock() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tart_time = time.clock()\n",
    "curr_time = time.clock()\n",
    "\n",
    "chunk_size = 100000 #because 10 answers for a question\n",
    "bm25_header_names = [\"q_id\",\"question\",\"answer\",\"label\",\"a_id\",\"bm25_score\"]\n",
    "trimmed_header_names = [\"question\",\"answer\",\"label\"]\n",
    "db_path  = \"data.db\"\n",
    "rearranged_table_name = \"dataset_rearranged\"\n",
    "selected_table_name = \"dataset_selected2\"\n",
    "tsv_op = \"../res/dataset.tsv\"\n",
    "\n",
    "total_rows = 5241880\n",
    "\n",
    "# sql_read_query = \"SELECT * FROM \"+ rearranged_table_name # +\" ORDER BY bm25_score\"\n",
    "sql_read_query = \"SELECT * FROM \"+ rearranged_table_name# +\" ORDER BY question , bm25_score\"\n",
    "sql_drop_query = \"DROP TABLE \" + rearranged_table_name\n",
    "\n",
    "\n",
    "con = None\n",
    "try:\n",
    "    skip_count = 0\n",
    "    file_read = 0\n",
    "    counter = 0\n",
    "    \n",
    "    tmp = 0\n",
    "    con = lite.connect(db_path)\n",
    "    print(\"SQLite connection established\")\n",
    "\n",
    "    for chunk in pd.read_sql_query(sql=sql_read_query, con=con,chunksize=chunk_size):\n",
    "        file_read += chunk.shape[0]\n",
    "        #if file_read<5200000:\n",
    "        #  continue\n",
    "        datalist = []\n",
    "\n",
    "        for chunklet in np.array_split(chunk, (chunk.shape[0]/10)):\n",
    "            chunklet = chunklet.reset_index()\n",
    "            true_index = chunklet.index[chunklet['label'] == 1].tolist()\n",
    "            for true_id in true_index:\n",
    "                unicodedata.normalize('NFD', chunklet.iloc[true_id]['question']).encode('ascii', 'ignore')\n",
    "                unicodedata.normalize('NFD', chunklet.iloc[true_id]['answer']).encode('ascii', 'ignore')\n",
    "                datalist.append(chunklet.iloc[true_id])\n",
    "                datalist.append(chunklet.iloc[true_id])\n",
    "                chunklet = chunklet.drop([true_id])\n",
    "            chunklet = chunklet.reset_index()\n",
    "            unicodedata.normalize('NFD', chunklet.iloc[0]['question']).encode('ascii', 'ignore')\n",
    "            unicodedata.normalize('NFD', chunklet.iloc[0]['answer']).encode('ascii', 'ignore')\n",
    "            \n",
    "            unicodedata.normalize('NFD', chunklet.iloc[1]['question']).encode('ascii', 'ignore')\n",
    "            unicodedata.normalize('NFD', chunklet.iloc[1]['answer']).encode('ascii', 'ignore')\n",
    "            datalist.append(chunklet.iloc[0])\n",
    "            datalist.append(chunklet.iloc[-1])\n",
    "            counter+=10\n",
    "\n",
    "        selected_chunk = pd.DataFrame(datalist, columns=bm25_header_names)\n",
    "        del selected_chunk['q_id']\n",
    "        del selected_chunk['a_id']\n",
    "        del selected_chunk['bm25_score']\n",
    "        selected_chunk.to_sql(name=selected_table_name, con=con, if_exists='append', index=False)\n",
    "        #selected_chunk.to_sql(name=selected_table_name, con=con, if_exists='append', index=False)\n",
    "        tmp += 1\n",
    "        #unicodedata.normalize('NFD', selected_chunk).encode('ascii', 'ignore')\n",
    "#         with open(tsv_op, 'a') as f:\n",
    "#             print(selected_chunk)\n",
    "#             selected_chunk.to_csv(f, sep='\\t', encoding='utf-8', index=False)\n",
    "\n",
    "        time_taken = time.clock() - curr_time\n",
    "        print(file_read,\" Done\\t Time taken : \",(time_taken), \"\\t ETA : \", (((total_rows-file_read)/chunk_size)*time_taken))\n",
    "        curr_time = time.clock()\n",
    "        #if tmp == 2 :\n",
    "            #break\n",
    "\n",
    "    # cursor = con.cursor()\n",
    "    # cursor.execute(sql_drop_query)\n",
    "    # con.commit()\n",
    "\n",
    "except lite.Error as e:\n",
    "    print(\"SQLite Error\")\n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.clock() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import unicodedata\n",
    "start_time = time.clock()\n",
    "curr_time = time.clock()\n",
    "\n",
    "chunk_size = 100000 #because 10 answers for a question\n",
    "trimmed_header_names = [\"question\",\"answer\",\"label\"]\n",
    "db_path  = \"data.db\"\n",
    "rearranged_table_name = \"dataset_rearranged\"\n",
    "selected_table_name = \"dataset_selected2\"\n",
    "tsv_op = \"../res/dataset.tsv\"\n",
    "\n",
    "total_rows = 5241880\n",
    "\n",
    "# sql_read_query = \"SELECT * FROM \"+ rearranged_table_name # +\" ORDER BY bm25_score\"\n",
    "sql_read_query = \"SELECT * FROM \"+ selected_table_name# +\" ORDER BY question , bm25_score\"\n",
    "sql_drop_query = \"DROP TABLE \" + rearranged_table_name\n",
    "encoding = \"utf-8\"\n",
    "\n",
    "\n",
    "\n",
    "con = None\n",
    "try:\n",
    "    skip_count = 0\n",
    "    file_read = 0\n",
    "    counter = 0\n",
    "    con = lite.connect(db_path)\n",
    "    print(\"SQLite connection established\")\n",
    "\n",
    "    for chunk in pd.read_sql_query(sql=sql_read_query, con=con,chunksize=chunk_size):\n",
    "        file_read += chunk.shape[0]\n",
    "        datalist = []\n",
    "        #print(chunk['label'])\n",
    "        for chunklet in chunk.iterrows():\n",
    "            chunklet = chunklet[1]\n",
    "            \n",
    "            #chunklet['question'] = unicodedata.normalize('NFD', chunklet['question']).decode(encoding).encode('ascii', 'ignore')\n",
    "            #chunklet['answer'] = unicodedata.normalize('NFD', chunklet['answer']).decode(encoding).encode('ascii', 'ignore')\n",
    "            chunklet['question'] = unicodedata.normalize('NFD', chunklet['question']).encode('ascii', 'ignore').decode(encoding)\n",
    "            chunklet['answer'] = unicodedata.normalize('NFD', chunklet['answer']).encode('ascii', 'ignore').decode(encoding)\n",
    "            #print(chunklet)\n",
    "            #exit(0)\n",
    "            datalist.append(chunklet)\n",
    "            \n",
    "        selected_chunk = pd.DataFrame(datalist, columns=trimmed_header_names)\n",
    "        with open(tsv_op, 'a') as f:\n",
    "            selected_chunk.to_csv(f, sep='\\t', encoding='utf-8', index=False, header = False)\n",
    "\n",
    "        time_taken = time.clock() - curr_time\n",
    "        print(file_read,\" Done\\t Time taken : \",(time_taken), \"\\t ETA : \", (((total_rows-file_read)/chunk_size)*time_taken))\n",
    "        curr_time = time.clock()\n",
    "\n",
    "    # cursor = con.cursor()\n",
    "    # cursor.execute(sql_drop_query)\n",
    "    # con.commit()\n",
    "\n",
    "except lite.Error as e:\n",
    "    print(\"SQLite Error\")\n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.clock() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fraction = 0.01\n",
    "start_time = time.clock()\n",
    "curr_time = time.clock()\n",
    "\n",
    "total_rows = 2096752 #approximately, if 4 answers for every question\n",
    "chunk_size = 100000\n",
    "tsv_path = \"../res/dataset.tsv\"\n",
    "subset = \"../res/datasubset.tsv\"\n",
    "header_names = [\"question\",\"answer\",\"label\"]\n",
    "file_read = 0\n",
    "\n",
    "for chunk in pd.read_csv(tsv_path,sep='\\t',chunksize=chunk_size, header=None, names=header_names, encoding='utf8'):\n",
    "    file_read += chunk.shape[0]\n",
    "    new_chunk = chunk.sample(frac=fraction).reset_index(drop=True)\n",
    "    \n",
    "    with open(subset, 'a') as f:\n",
    "            new_chunk.to_csv(f, sep='\\t', encoding='utf-8', index=False, header = False)\n",
    "    \n",
    "    time_taken = time.clock() - curr_time\n",
    "    print(file_read,\" Done\\t Time taken : \",(time_taken), \"\\t ETA : \", (((total_rows-file_read)/chunk_size)*time_taken))\n",
    "    curr_time = time.clock()\n",
    "    \n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.clock() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imbalance_report(df):\n",
    "    true_count = (df['label'] == 1).sum()\n",
    "    false_count = (df['label'] == 0).sum()\n",
    "    \n",
    "    print(df.label.unique())\n",
    "    #print(df['label'])\n",
    "    \n",
    "    total_count = true_count+false_count\n",
    "    print(\"true : \", (true_count/total_count))\n",
    "    print(\"false : \", (false_count/total_count))\n",
    "    #print(\"label : \", (label_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "curr_time = time.clock()\n",
    "\n",
    "test_frac = 0.2 # out of total\n",
    "valid_frac = 0.25 #out of test, i.e. 1- test\n",
    "\n",
    "total_rows = 2096752 #approximately, if 4 answers for every question\n",
    "chunk_size = 100000\n",
    "subset = \"../res/datasubset.tsv\"\n",
    "\n",
    "train_set = \"../res/train_set.tsv\"\n",
    "test_set = \"../res/test_set.tsv\"\n",
    "valid_set = \"../res/valid_set.tsv\"\n",
    "\n",
    "\n",
    "header_names = [\"question\",\"answer\",\"label\"]\n",
    "file_read = 0\n",
    "\n",
    "for chunk in pd.read_csv(subset, sep='\\t', chunksize=chunk_size, header=None, names=header_names, encoding='utf8'):\n",
    "    file_read += chunk.shape[0]\n",
    "    get_imbalance_report(chunk)\n",
    "    train_chunk, test_chunk = train_test_split(chunk, test_size=test_frac)\n",
    "    \n",
    "    train_chunk, valid_chunk = train_test_split(train_chunk, test_size=valid_frac)\n",
    "    \n",
    "    with open(train_set, 'a') as f:\n",
    "            train_chunk.to_csv(f, sep='\\t', encoding='utf-8', index=False, header = False)\n",
    "    \n",
    "    with open(test_set, 'a') as f:\n",
    "            test_chunk.to_csv(f, sep='\\t', encoding='utf-8', index=False, header = False)\n",
    "            \n",
    "    with open(valid_set, 'a') as f:\n",
    "            valid_chunk.to_csv(f, sep='\\t', encoding='utf-8', index=False, header = False)\n",
    "    \n",
    "    time_taken = time.clock() - curr_time\n",
    "    print(file_read,\" Done\\t Time taken : \",(time_taken), \"\\t ETA : \", (((total_rows-file_read)/chunk_size)*time_taken))\n",
    "    curr_time = time.clock()\n",
    "    \n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.clock() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "curr_time = time.clock()\n",
    "\n",
    "test_frac = 0.2 # out of total\n",
    "valid_frac = 0.25 #out of test, i.e. 1- test\n",
    "\n",
    "total_rows = 2096752 #approximately, if 4 answers for every question\n",
    "chunk_size = 100000\n",
    "subset = \"../res/datasubset.tsv\"\n",
    "\n",
    "train_set = \"../res/train_set.tsv\"\n",
    "test_set = \"../res/test_set.tsv\"\n",
    "valid_set = \"../res/valid_set.tsv\"\n",
    "\n",
    "\n",
    "header_names = [\"question\",\"answer\",\"label\"]\n",
    "file_read = 0\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "print(\"\\n\\n\\nTrain Chunk\\n\")\n",
    "train_chunk = pd.read_csv(train_set, sep='\\t', header=None, names=header_names, encoding='utf8')\n",
    "get_imbalance_report(train_chunk)\n",
    "\n",
    "print(\"\\n\\n\\nTest Chunk\\n\")\n",
    "test_chunk = pd.read_csv(test_set, sep='\\t', header=None, names=header_names, encoding='utf8')\n",
    "get_imbalance_report(test_chunk)\n",
    "\n",
    "print(\"\\n\\n\\nValid Chunk\\n\")\n",
    "valid_chunk = pd.read_csv(valid_set, sep='\\t', header=None, names=header_names, encoding='utf8')\n",
    "get_imbalance_report(valid_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
