{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import random\n",
    "import csv\n",
    "import sqlite3 as lite\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "#train_on_gpu = False\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    \n",
    "train_set = \"../res/train_set.tsv\"\n",
    "test_set = \"../res/test_set.tsv\"\n",
    "valid_set = \"../res/valid_set.tsv\"\n",
    "ft_vec = \"../res/crawl-300d-2M-subword.vec\"\n",
    "glove50_path = \"../res/GloveDict50.pkl\"\n",
    "\n",
    "model_path = \"../res/mod3.pth\"\n",
    "\n",
    "max_q_len = 10\n",
    "max_a_len = 100\n",
    "embedding_len = 50\n",
    "pad_char = '_'\n",
    "batch_size=32\n",
    "header_names = [\"question\",\"answer\",\"label\"]\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(fname):\n",
    "    pickle_in = open(fname,\"rb\")\n",
    "    _dict = pickle.load(pickle_in)\n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove50 = load_glove_vectors(glove50_path)\n",
    "supported_words = list(glove50.keys())\n",
    "#print(supported_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_val = np.zeros(embedding_len)\n",
    "glove = defaultdict(lambda: default_val,glove50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetFromCSV(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        _dataset = pd.read_csv(csv_path, sep='\\t', header=None, names=header_names, encoding='utf8')\n",
    "        self.labels = _dataset['label']\n",
    "        self.questions = _dataset['question']\n",
    "        self.answers = _dataset['answer']\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        q = self.questions.iloc[index]\n",
    "        a = self.answers.iloc[index]\n",
    "        q = q.lower()\n",
    "        a = a.lower()\n",
    "        q = q.translate(translator)\n",
    "        a = a.translate(translator)\n",
    "        \n",
    "        tokens = set((q+\" \"+a).split(\" \"))\n",
    "        unique_vals = [x for x in tokens if x not in supported_words]      \n",
    "        \n",
    "        \n",
    "        q_list = np.zeros((max_q_len, (embedding_len+1)))\n",
    "        a_list = np.zeros((max_a_len, (embedding_len+1)))\n",
    "        \n",
    "        \n",
    "        q_words = q.split(\" \")        \n",
    "        if len(q_words) > max_q_len:\n",
    "            q_words = q_words[:max_q_len]\n",
    "        else:\n",
    "            q_words = [pad_char]*(max_q_len - len(q_words) ) + q_words\n",
    "            \n",
    "        a_words = a.split(\" \")\n",
    "        if len(a_words) > max_a_len:\n",
    "            a_words = a_words[:max_a_len]\n",
    "        else:\n",
    "            a_words = [pad_char]*(max_a_len - len(a_words) ) + a_words\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i, word in enumerate(q_words):\n",
    "            q_val = glove[word]\n",
    "            if word not in unique_vals:\n",
    "                q_val = np.append(q_val, 0)\n",
    "            else:\n",
    "                q_val = np.append(q_val, unique_vals.index(word))            \n",
    "            q_list[i] = q_val\n",
    "        \n",
    "            \n",
    "        for i, word in enumerate(a_words):\n",
    "            a_val = glove[word]\n",
    "            if word not in unique_vals:\n",
    "                a_val = np.append(a_val, 0)\n",
    "            else:\n",
    "                a_val = np.append(a_val, unique_vals.index(word))            \n",
    "            a_list[i] = a_val\n",
    "        \n",
    "        \n",
    "        x = [q_list,a_list]\n",
    "        y = self.labels.iloc[index]\n",
    "        return x, y\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class W_RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_hidden=128, n_layers=3, drop_prob=0.2, lr=0.001):\n",
    "        super(W_RNN,self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "\n",
    "        self.lstm = nn.LSTM((embedding_len+1), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, (embedding_len+1))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network.\n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        x = x.float()\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "         # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        #out = self.sigmoid(x)\n",
    "        return out, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "\n",
    "        return hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_OP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_layers, drop_p=0.5):\n",
    "        ''' Builds a feedforward network with arbitrary hidden layers.\n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            input_size: integer, size of the input layer\n",
    "            output_size: integer, size of the output layer\n",
    "            hidden_layers: list of integers, the sizes of the hidden layers\n",
    "        \n",
    "        '''\n",
    "        super(FC_OP, self).__init__()\n",
    "        # Input to a hidden layer\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
    "        \n",
    "        # Add a variable number of more hidden layers\n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
    "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "        \n",
    "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=drop_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        for each in self.hidden_layers:\n",
    "            x = F.relu(each(x))\n",
    "            x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        #print(\"Before \",x.size())\n",
    "        x = F.sigmoid(x)\n",
    "        #print(\"After \",x.size())\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.qNet = W_RNN()\n",
    "        self.aNet = W_RNN()\n",
    "        self.fcNet = FC_OP(input_size = ((embedding_len+1)*2),\n",
    "                          output_size = 1,\n",
    "                          hidden_layers = [128,128], \n",
    "                          drop_p = 0.2)\n",
    "\n",
    "    def forward(self, questions, answers, q_hidden, a_hidden):\n",
    "        q, q_hidden = self.qNet(questions, q_hidden)\n",
    "        a, a_hidden = self.aNet(answers, a_hidden)\n",
    "        #print(q.size())\n",
    "        #print(a.size())\n",
    "        ops = torch.cat((q, a), 1)                \n",
    "        output = self.fcNet(ops)\n",
    "        return output, q_hidden, a_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        q_hidden = self.qNet.init_hidden(batch_size)\n",
    "        a_hidden = self.aNet.init_hidden(batch_size)\n",
    "        return q_hidden, a_hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model loss and optimizers here\n",
    "net = Model()\n",
    "#print(net)\n",
    "\n",
    "lr = 0.0001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "if train_on_gpu:\n",
    "    net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data = CustomDatasetFromCSV(test_set)    \n",
    "test_loader = torch.utils.data.DataLoader(dataset=custom_data, batch_size=32, shuffle=True)\n",
    "\n",
    "custom_data = CustomDatasetFromCSV(train_set)    \n",
    "train_loader = torch.utils.data.DataLoader(dataset=custom_data, batch_size=32, shuffle=True)\n",
    "\n",
    "custom_data = CustomDatasetFromCSV(valid_set)    \n",
    "valid_loader = torch.utils.data.DataLoader(dataset=custom_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this for both test and validation set\n",
    "def test(dataloader):\n",
    "    dataset_loss = 0\n",
    "    dataset_acc = 0\n",
    "    net.eval()\n",
    "    count = 0    \n",
    "    batches_so_far = 0\n",
    "    elements_seen = 0\n",
    "    good_case = False\n",
    "    gc_record =[]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:            \n",
    "            q = x[0]\n",
    "            a = x[1]\n",
    "            if train_on_gpu:\n",
    "                q = q.cuda()\n",
    "                a = a.cuda()\n",
    "                y = y.cuda()                \n",
    "            l = len(y)\n",
    "            if l != batch_size:\n",
    "                # last incomplete batch, skip over it.\n",
    "                continue;\n",
    "            batches_so_far += 1\n",
    "            count += 1\n",
    "            elements_seen += l\n",
    "            q_hidden, a_hidden = net.init_hidden(batch_size)            \n",
    "            output, q_hidden, a_hidden = net(questions = q, answers = a, q_hidden = q_hidden, a_hidden = a_hidden)\n",
    "            output = output.flatten()\n",
    "            loss = criterion(output, y.float())\n",
    "            interpretation = (output>0.5).float()\n",
    "            i_sum = interpretation.sum()\n",
    "            if i_sum == 0 or i_sum == l:\n",
    "                #print(\"!\", end = \"\")\n",
    "                pass\n",
    "            else:\n",
    "                good_case = True\n",
    "                gc_record.append(i_sum)\n",
    "                #print(interpretation)\n",
    "                ## print(\"True Flag \", interpretation.sum())\n",
    "            equals = interpretation == y.float()           \n",
    "            dataset_loss += loss.item()\n",
    "            dataset_acc += equals.sum()\n",
    "            \n",
    "        dataset_loss = dataset_loss/batches_so_far\n",
    "        dataset_accuracy = dataset_acc.item()/elements_seen\n",
    "        \n",
    "        if good_case:\n",
    "            print(\"!!!GOODCASE!!!\\n\",gc_record,\"\\n\")\n",
    "            gc_record = []\n",
    "            good_case = False\n",
    "        \n",
    "    return dataset_loss, dataset_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_this(d1, d2, name):\n",
    "    plt.figure(name)\n",
    "    lists1 = d1.items() # sorted by key, return a list of tuples\n",
    "    lists2 = d2.items() # sorted by key, return a list of tuples\n",
    "    x1, y1 = zip(*lists1) # unpack a list of pairs into two tuples\n",
    "    x2, y2 = zip(*lists2) # unpack a list of pairs into two tuples\n",
    "    plt.plot(x1, y1, label = \"train\")\n",
    "    plt.plot(x2, y2, label = \"valid\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtl = {}\n",
    "dta = {}   \n",
    "dvl = {}\n",
    "dva = {}\n",
    "\n",
    "lowest_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch=1, print_every=1, clip = 5):\n",
    "    net.train()\n",
    "    train_loss = 0;\n",
    "    train_acc = 0;\n",
    "    valid_loss = 0;\n",
    "    valid_acc = 0;\n",
    "    count = 0    \n",
    "    \n",
    "    dtl = {}\n",
    "    dta = {}   \n",
    "    dvl = {}\n",
    "    dva = {}\n",
    "    \n",
    "    interrupted = False\n",
    "\n",
    "    lowest_loss = np.inf\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        \n",
    "        batches_so_far = 0\n",
    "        elements_seen = 0\n",
    "        for x, y in train_loader:            \n",
    "            q = x[0]\n",
    "            a = x[1]\n",
    "            if train_on_gpu:\n",
    "                q = q.cuda()\n",
    "                a = a.cuda()\n",
    "                y = y.cuda()                \n",
    "            l = len(y)      \n",
    "            if interrupted:\n",
    "                interrupted = False\n",
    "                print(\"Continue as usual again\")\n",
    "            \n",
    "            if l != batch_size:\n",
    "                print(\"Abrupt batch size for training, l = \", l)\n",
    "                print(\"epoch \", e)\n",
    "                print(\"count \", count)\n",
    "                print(\"batches_so_far \", batches_so_far)\n",
    "                print(\"elements_seen \", elements_seen)\n",
    "                interrupted = True\n",
    "                \n",
    "                # last incomplete batch, skip over it.\n",
    "                continue;\n",
    "            elements_seen += l # not one, but l\n",
    "            batches_so_far +=1\n",
    "            count += 1            \n",
    "            net.zero_grad()\n",
    "            q_hidden, a_hidden = net.init_hidden(batch_size)            \n",
    "            output, q_hidden, a_hidden = net(questions = q, answers = a, q_hidden = q_hidden, a_hidden = a_hidden)\n",
    "            \n",
    "            #output = output.flatten()\n",
    "            #print(\"\\n\\n\\noutput \",output.flatten())\n",
    "            #print(\"\\n\\n\\noutput \",output.size())\n",
    "            #print(\"\\ny.float() \",y.size() )\n",
    "            y = y.view(batch_size, -1)\n",
    "            #print(\"\\ny.float() \",y.size() )\n",
    "            #print(\"\\nmodel path \",model_path )\n",
    "            \n",
    "            loss = criterion(output, y.float())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "            interpretation = (output>0.5).float()\n",
    "            equals = interpretation == y.float()            \n",
    "            train_loss += loss.item()\n",
    "            train_acc += equals.sum()\n",
    "            #print(\"\\ninterpretation \",interpretation)\n",
    "            #print(\"\\ny.float() \",y.float() )\n",
    "            #print(\"\\ntrain ac\",train_acc)\n",
    "            #print(\"\\elements_seen \",elements_seen)\n",
    "            #print(\"\\equals \",equals)\n",
    "            #print(\"\\sum \",equals.sum())\n",
    "            \n",
    "            if count%print_every == 0:\n",
    "                # why counts are batch size and not int\n",
    "                train_loss = train_loss/batches_so_far\n",
    "                #print(\"\\n1 train ac\",train_acc)\n",
    "                train_accuracy = train_acc.item()/elements_seen                \n",
    "                #print(\"\\n2 train ac\",train_acc)\n",
    "                \n",
    "                # to save some time at initial training\n",
    "                # calculate validation only after 1000 batches\n",
    "                #if count > 200:\n",
    "                valid_loss, valid_accuracy = test(valid_loader)\n",
    "                #else:\n",
    "                #valid_loss = train_loss\n",
    "                #valid_accuracy = train_accuracy\n",
    "                    \n",
    "                clear_output(wait=True)\n",
    "                print(\"Train Loss \", train_loss, \"\\tTrain Acc \", train_accuracy, \"\\nValid Loss\", valid_loss, \"\\tValid Acc\", valid_accuracy)\n",
    "                if  valid_loss < lowest_loss:\n",
    "                    lowest_loss = valid_loss\n",
    "                    \n",
    "                    torch.save(net.state_dict(),model_path)                \n",
    "                net.train()                    \n",
    "                dtl[count]= train_loss\n",
    "                dvl[count]= valid_loss\n",
    "                dta[count]= train_accuracy\n",
    "                dva[count]= valid_accuracy\n",
    "                plot_this(dtl, dvl, \"Loss\")\n",
    "                plot_this(dta, dva, \"Acc\")\n",
    "                \n",
    "                batches_so_far = 0\n",
    "                elements_seen = 0\n",
    "                train_loss = 0;\n",
    "                train_acc = 0;\n",
    "                valid_loss = 0;\n",
    "                valid_acc = 0;\n",
    "                \n",
    "                #if count == 5: break\n",
    "                \n",
    "            #break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epoch = 10, print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple load\n",
    "state_dict_1 = torch.load(model_path)\n",
    "\n",
    "# simple load into model\n",
    "net.load_state_dict(state_dict_1)\n",
    "if train_on_gpu:\n",
    "    net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = test(test_loader)\n",
    "\n",
    "#clear_output(wait=True)\n",
    "print(\"test_loss \", test_loss, \"\\ttest_accuracy \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
