{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import random\n",
    "import csv\n",
    "import sqlite3 as lite\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "#train_on_gpu = False\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    \n",
    "train_set = \"../res/train_set.tsv\"\n",
    "test_set = \"../res/test_set.tsv\"\n",
    "valid_set = \"../res/valid_set.tsv\"\n",
    "glove50_path = \"../res/GloveDict50.pkl\"\n",
    "\n",
    "model_path = \"../res/mod4.pth\"\n",
    "\n",
    "max_q_len = 10\n",
    "max_a_len = 100\n",
    "embedding_len = 50\n",
    "pad_char = '_'\n",
    "batch_size=32\n",
    "header_names = [\"question\",\"answer\",\"label\"]\n",
    "\n",
    "e_unknown = 0\n",
    "e_train = 1\n",
    "e_valid = 2\n",
    "e_test = 3\n",
    "\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(fname):\n",
    "    pickle_in = open(fname,\"rb\")\n",
    "    _dict = pickle.load(pickle_in)\n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove50 = load_glove_vectors(glove50_path)\n",
    "supported_words = list(glove50.keys())\n",
    "#print(supported_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_val = np.zeros(embedding_len)\n",
    "glove = defaultdict(lambda: default_val,glove50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetFromCSV(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        _dataset = pd.read_csv(csv_path, sep='\\t', header=None, names=header_names, encoding='utf8')\n",
    "        self.labels = _dataset['label']\n",
    "        self.questions = _dataset['question']\n",
    "        self.answers = _dataset['answer']\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        q = self.questions.iloc[index]\n",
    "        a = self.answers.iloc[index]\n",
    "        q = q.lower()\n",
    "        a = a.lower()\n",
    "        q = q.translate(translator)\n",
    "        a = a.translate(translator)\n",
    "        \n",
    "        tokens = set((q+\" \"+a).split(\" \"))\n",
    "        unique_vals = [x for x in tokens if x not in supported_words]      \n",
    "        \n",
    "        \n",
    "        q_list = np.zeros((max_q_len, (embedding_len+1)))\n",
    "        a_list = np.zeros((max_a_len, (embedding_len+1)))\n",
    "        \n",
    "        \n",
    "        q_words = q.split(\" \")        \n",
    "        if len(q_words) > max_q_len:\n",
    "            q_words = q_words[:max_q_len]\n",
    "        else:\n",
    "            q_words = [pad_char]*(max_q_len - len(q_words) ) + q_words\n",
    "            \n",
    "        a_words = a.split(\" \")\n",
    "        if len(a_words) > max_a_len:\n",
    "            a_words = a_words[:max_a_len]\n",
    "        else:\n",
    "            a_words = [pad_char]*(max_a_len - len(a_words) ) + a_words\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i, word in enumerate(q_words):\n",
    "            q_val = glove[word]\n",
    "            if word not in unique_vals:\n",
    "                q_val = np.append(q_val, 0)\n",
    "            else:\n",
    "                q_val = np.append(q_val, unique_vals.index(word))            \n",
    "            q_list[i] = q_val\n",
    "        \n",
    "            \n",
    "        for i, word in enumerate(a_words):\n",
    "            a_val = glove[word]\n",
    "            if word not in unique_vals:\n",
    "                a_val = np.append(a_val, 0)\n",
    "            else:\n",
    "                a_val = np.append(a_val, unique_vals.index(word))            \n",
    "            a_list[i] = a_val\n",
    "        \n",
    "        \n",
    "        x = [q_list,a_list]\n",
    "        y = self.labels.iloc[index]\n",
    "        return x, y\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class W_RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_hidden=128, n_layers=3, drop_prob=0.2, lr=0.001):\n",
    "        super(W_RNN,self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "\n",
    "        self.lstm = nn.LSTM((embedding_len+1), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, (embedding_len+1))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network.\n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        x = x.float()\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "         # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        #out = self.sigmoid(x)\n",
    "        return out, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "\n",
    "        return hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_OP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_layers, drop_p=0.5):\n",
    "        ''' Builds a feedforward network with arbitrary hidden layers.\n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            input_size: integer, size of the input layer\n",
    "            output_size: integer, size of the output layer\n",
    "            hidden_layers: list of integers, the sizes of the hidden layers\n",
    "        \n",
    "        '''\n",
    "        super(FC_OP, self).__init__()\n",
    "        # Input to a hidden layer\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
    "        \n",
    "        # Add a variable number of more hidden layers\n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
    "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "        \n",
    "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=drop_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        for each in self.hidden_layers:\n",
    "            x = F.relu(each(x))\n",
    "            x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        #print(\"Before \",x.size())\n",
    "        x = F.sigmoid(x)\n",
    "        #print(\"After \",x.size())\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.qNet = W_RNN()\n",
    "        self.aNet = W_RNN()\n",
    "        self.fcNet = FC_OP(input_size = ((embedding_len+1)*2),\n",
    "                          output_size = 1,\n",
    "                          hidden_layers = [128,128], \n",
    "                          drop_p = 0.2)\n",
    "\n",
    "    def forward(self, questions, answers, q_hidden, a_hidden):\n",
    "        q, q_hidden = self.qNet(questions, q_hidden)\n",
    "        a, a_hidden = self.aNet(answers, a_hidden)\n",
    "        #print(q.size())\n",
    "        #print(a.size())\n",
    "        ops = torch.cat((q, a), 1)                \n",
    "        output = self.fcNet(ops)\n",
    "        return output, q_hidden, a_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        q_hidden = self.qNet.init_hidden(batch_size)\n",
    "        a_hidden = self.aNet.init_hidden(batch_size)\n",
    "        return q_hidden, a_hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model loss and optimizers here\n",
    "net = Model()\n",
    "#print(net)\n",
    "\n",
    "lr = 0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "if train_on_gpu:\n",
    "    net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1.],[0.],[0.],[1.],[0.]])\n",
    "b = torch.tensor([[1.],[0.],[0.],[1.],[0.]])\n",
    "l = criterion(a,b)\n",
    "print(l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data = CustomDatasetFromCSV(test_set)    \n",
    "test_loader = torch.utils.data.DataLoader(dataset=custom_data, batch_size=32, shuffle=True)\n",
    "\n",
    "custom_data = CustomDatasetFromCSV(train_set)    \n",
    "train_loader = torch.utils.data.DataLoader(dataset=custom_data, batch_size=32, shuffle=True)\n",
    "\n",
    "custom_data = CustomDatasetFromCSV(valid_set)    \n",
    "valid_loader = torch.utils.data.DataLoader(dataset=custom_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(yhat, y):\n",
    "    yhat = yhat.flatten()\n",
    "    y = y.flatten()  \n",
    "    if yhat.size() != y.size():\n",
    "        print(\"dimension mismatch\")\n",
    "        return 0, [], []\n",
    "    \n",
    "    interpretation = (yhat>0.5).float()\n",
    "    equals = interpretation == y.float()\n",
    "    #print(\"equals.sum() \", equals.sum().item())\n",
    "    #print(\"y.size() \", len(y))\n",
    "    accuracy = equals.sum().item()/len(y)\n",
    "    return accuracy, interpretation, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment='A')\n",
    "\n",
    "tag_train_loss = \"train_loss\"\n",
    "tag_train_acc = \"train_acc\"\n",
    "\n",
    "tag_valid_loss = \"valid_loss\"\n",
    "tag_valid_acc = \"valid_acc\"\n",
    "\n",
    "tag_train_f1score = \"train_f1score\"\n",
    "tag_valid_f1score = \"valid_f1score\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this for both test and validation set\n",
    "def test(dataloader, iteration):\n",
    "    dataset_loss = 0\n",
    "    dataset_acc = 0\n",
    "    net.eval()\n",
    "    count = iteration    \n",
    "    batches_so_far = 0\n",
    "    elements_seen = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:            \n",
    "            q = x[0]\n",
    "            a = x[1]\n",
    "            if train_on_gpu:\n",
    "                q = q.cuda()\n",
    "                a = a.cuda()\n",
    "                y = y.cuda()                \n",
    "            l = len(y)\n",
    "            if l != batch_size:\n",
    "                # last incomplete batch, skip over it.\n",
    "                continue;\n",
    "            batches_so_far += 1\n",
    "            count += 1\n",
    "            elements_seen += l\n",
    "            q_hidden, a_hidden = net.init_hidden(batch_size)            \n",
    "            output, q_hidden, a_hidden = net(questions = q, answers = a, q_hidden = q_hidden, a_hidden = a_hidden)\n",
    "            output = output.flatten()\n",
    "            loss = criterion(output, y.float())                 \n",
    "            \n",
    "            dataset_acc_val, pred_val ,actual_val = get_acc(output, y)            \n",
    "            f1score = f1_score(pred_val.cpu().data.numpy(), actual_val.cpu().data.numpy())\n",
    "            writer.add_scalar(tag_valid_loss, loss.item(), count)\n",
    "            writer.add_scalar(tag_valid_acc, dataset_acc_val, count)\n",
    "            writer.add_scalar(tag_valid_f1score, f1score, count)           \n",
    "            \n",
    "                        \n",
    "            dataset_loss += loss.item()\n",
    "            dataset_acc += dataset_acc_val\n",
    "            \n",
    "        dataset_loss = dataset_loss/batches_so_far\n",
    "        dataset_accuracy = dataset_acc/batches_so_far\n",
    "        \n",
    "    return dataset_loss, dataset_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch=1, print_every=1, clip = 5):\n",
    "    net.train()\n",
    "    train_loss = 0;\n",
    "    train_acc = 0;\n",
    "    valid_loss = 0;\n",
    "    valid_acc = 0;\n",
    "    count = 0  \n",
    "    \n",
    "    final_train_loss = 0;\n",
    "    final_train_acc = 0;\n",
    "    final_valid_loss = 0;\n",
    "    final_valid_acc = 0;\n",
    "    \n",
    "    \n",
    "    interrupted = False\n",
    "\n",
    "    lowest_loss = np.inf\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        \n",
    "        batches_so_far = 0\n",
    "        elements_seen = 0\n",
    "        for x, y in train_loader:            \n",
    "            q = x[0]\n",
    "            a = x[1]\n",
    "            if train_on_gpu:\n",
    "                q = q.cuda()\n",
    "                a = a.cuda()\n",
    "                y = y.cuda()                \n",
    "            l = len(y)      \n",
    "            if interrupted:\n",
    "                interrupted = False\n",
    "                print(\"Continue as usual again\")\n",
    "            \n",
    "            if l != batch_size:\n",
    "                print(\"Abrupt batch size for training, l = \", l)\n",
    "                print(\"epoch \", e)\n",
    "                print(\"count \", count)\n",
    "                print(\"batches_so_far \", batches_so_far)\n",
    "                print(\"elements_seen \", elements_seen)\n",
    "                interrupted = True                \n",
    "                # last incomplete batch, skip over it.\n",
    "                continue;\n",
    "            \n",
    "            elements_seen += l # not one, but l\n",
    "            batches_so_far +=1\n",
    "            count += 1\n",
    "            \n",
    "            net.zero_grad()\n",
    "            q_hidden, a_hidden = net.init_hidden(batch_size)            \n",
    "            output, q_hidden, a_hidden = net(questions = q, answers = a, q_hidden = q_hidden, a_hidden = a_hidden)\n",
    "                        \n",
    "            #y = y.view(batch_size, -1)\n",
    "            output = output.flatten()\n",
    "            \n",
    "            loss = criterion(output, y.float())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()                     \n",
    "\n",
    "            train_acc_val, pred_val ,actual_val = get_acc(output, y)\n",
    "            f1score = f1_score(pred_val.cpu().data.numpy(), actual_val.cpu().data.numpy())\n",
    "            writer.add_scalar(tag_train_loss, loss.item(), count)\n",
    "            writer.add_scalar(tag_train_acc, train_acc_val, count)\n",
    "            writer.add_scalar(tag_train_f1score, f1score, count)\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_acc += train_acc_val\n",
    "                \n",
    "            if count%print_every == 0:\n",
    "                final_train_loss = train_loss/batches_so_far\n",
    "                if final_train_loss > 1:\n",
    "                    print(\"Loss ERROR: train_loss \", train_loss,\"\\t - \\t\" , batches_so_far)                \n",
    "                \n",
    "                final_train_acc = train_acc/batches_so_far \n",
    "                if final_train_acc > 1:\n",
    "                    print(\"train_accuracy \", train_acc,\"\\t - \\t\" , elements_seen)                \n",
    "                \n",
    "                valid_loss, valid_acc = test(valid_loader, iteration=count)\n",
    "                \n",
    "                final_valid_acc = valid_acc\n",
    "                final_valid_loss = valid_loss                \n",
    "                                              \n",
    "                \n",
    "                clear_output(wait=True)                \n",
    "                print(\"Count \", count, \"\\t Epoch \", e)\n",
    "                print(\"Train Loss \", final_train_loss, \"\\tTrain Acc \", final_train_acc, \"\\nValid Loss\", final_valid_loss, \"\\tValid Acc\", final_valid_acc)\n",
    "                \n",
    "                \n",
    "                if  valid_loss < lowest_loss:\n",
    "                    lowest_loss = valid_loss\n",
    "                    torch.save(net.state_dict(),model_path)                \n",
    "                net.train()                    \n",
    "        \n",
    "\n",
    "                batches_so_far = 0\n",
    "                elements_seen = 0\n",
    "                train_loss = 0;\n",
    "                train_acc = 0;\n",
    "                valid_loss = 0;\n",
    "                valid_acc = 0;\n",
    "                \n",
    "                #if count == 5: break\n",
    "                \n",
    "            #break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loss here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epoch = 10, print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple load\n",
    "state_dict_1 = torch.load(model_path)\n",
    "\n",
    "# simple load into model\n",
    "net.load_state_dict(state_dict_1)\n",
    "if train_on_gpu:\n",
    "    net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = test(test_loader)\n",
    "\n",
    "#clear_output(wait=True)\n",
    "print(\"test_loss \", test_loss, \"\\ttest_accuracy \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
