{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import random\n",
    "import csv\n",
    "import sqlite3 as lite\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "#train_on_gpu = False\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    \n",
    "train_set = \"../res/train_set.tsv\"\n",
    "test_set = \"../res/test_set.tsv\"\n",
    "valid_set = \"../res/valid_set.tsv\"\n",
    "ft_vec = \"../res/crawl-300d-2M-subword.vec\"\n",
    "glove50_path = \"../res/GloveDict50.pkl\"\n",
    "\n",
    "model_path = \"../res/mod1.pth\"\n",
    "\n",
    "max_q_len = 10\n",
    "max_a_len = 100\n",
    "embedding_len = 50\n",
    "pad_char = '_'\n",
    "batch_size=32\n",
    "header_names = [\"question\",\"answer\",\"label\"]\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(fname):\n",
    "    pickle_in = open(fname,\"rb\")\n",
    "    _dict = pickle.load(pickle_in)\n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove50 = load_glove_vectors(glove50_path)\n",
    "supported_words = list(glove50.keys())\n",
    "#print(supported_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_val = np.zeros(embedding_len)\n",
    "glove = defaultdict(lambda: default_val,glove50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetFromCSV(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        _dataset = pd.read_csv(csv_path, sep='\\t', header=None, names=header_names, encoding='utf8')\n",
    "        self.labels = _dataset['label']\n",
    "        self.questions = _dataset['question']\n",
    "        self.answers = _dataset['answer']\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        q = self.questions.iloc[index]\n",
    "        a = self.answers.iloc[index]\n",
    "        q = q.lower()\n",
    "        a = a.lower()\n",
    "        q = q.translate(translator)\n",
    "        a = a.translate(translator)\n",
    "        \n",
    "        tokens = set((q+\" \"+a).split(\" \"))\n",
    "        unique_vals = [x for x in tokens if x not in supported_words]      \n",
    "        \n",
    "        \n",
    "        q_list = np.zeros((max_q_len, (embedding_len+1)))\n",
    "        a_list = np.zeros((max_a_len, (embedding_len+1)))\n",
    "        \n",
    "        \n",
    "        q_words = q.split(\" \")        \n",
    "        if len(q_words) > max_q_len:\n",
    "            q_words = q_words[:max_q_len]\n",
    "        else:\n",
    "            q_words = [pad_char]*(max_q_len - len(q_words) ) + q_words\n",
    "            \n",
    "        a_words = a.split(\" \")\n",
    "        if len(a_words) > max_a_len:\n",
    "            a_words = a_words[:max_a_len]\n",
    "        else:\n",
    "            a_words = [pad_char]*(max_a_len - len(a_words) ) + a_words\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i, word in enumerate(q_words):\n",
    "            q_val = glove[word]\n",
    "            if word not in unique_vals:\n",
    "                q_val = np.append(q_val, 0)\n",
    "            else:\n",
    "                q_val = np.append(q_val, unique_vals.index(word))            \n",
    "            q_list[i] = q_val\n",
    "        \n",
    "            \n",
    "        for i, word in enumerate(a_words):\n",
    "            a_val = glove[word]\n",
    "            if word not in unique_vals:\n",
    "                a_val = np.append(a_val, 0)\n",
    "            else:\n",
    "                a_val = np.append(a_val, unique_vals.index(word))            \n",
    "            a_list[i] = a_val\n",
    "        \n",
    "        \n",
    "        x = [q_list,a_list]\n",
    "        y = self.labels.iloc[index]\n",
    "        return x, y\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class W_RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_hidden=128, n_layers=3, drop_prob=0.2, lr=0.001):\n",
    "        super(W_RNN,self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "\n",
    "        self.lstm = nn.LSTM((embedding_len+1), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, (embedding_len+1))\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network.\n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        x = x.float()\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "         # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "\n",
    "        return hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_OP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_layers, drop_p=0.5):\n",
    "        ''' Builds a feedforward network with arbitrary hidden layers.\n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            input_size: integer, size of the input layer\n",
    "            output_size: integer, size of the output layer\n",
    "            hidden_layers: list of integers, the sizes of the hidden layers\n",
    "        \n",
    "        '''\n",
    "        super(FC_OP, self).__init__()\n",
    "        # Input to a hidden layer\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
    "        \n",
    "        # Add a variable number of more hidden layers\n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
    "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "        \n",
    "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=drop_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        for each in self.hidden_layers:\n",
    "            x = F.relu(each(x))\n",
    "            x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.qNet = W_RNN()\n",
    "        self.aNet = W_RNN()\n",
    "        self.fcNet = FC_OP(input_size = ((embedding_len+1)*2),\n",
    "                          output_size = 1,\n",
    "                          hidden_layers = [128,128], \n",
    "                          drop_p = 0.2)\n",
    "\n",
    "    def forward(self, questions, answers, q_hidden, a_hidden):\n",
    "        q, q_hidden = self.qNet(questions, q_hidden)\n",
    "        a, a_hidden = self.aNet(answers, a_hidden)        \n",
    "        ops = torch.cat((q, a), 1)                \n",
    "        output = self.fcNet(ops)\n",
    "        return output, q_hidden, a_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        q_hidden = self.qNet.init_hidden(batch_size)\n",
    "        a_hidden = self.aNet.init_hidden(batch_size)\n",
    "        return q_hidden, a_hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model loss and optimizers here\n",
    "net = Model()\n",
    "#print(net)\n",
    "\n",
    "lr = 0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "if train_on_gpu:\n",
    "    net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data = CustomDatasetFromCSV(test_set)    \n",
    "test_loader = torch.utils.data.DataLoader(dataset=custom_data, batch_size=32, shuffle=True)\n",
    "\n",
    "custom_data = CustomDatasetFromCSV(train_set)    \n",
    "train_loader = torch.utils.data.DataLoader(dataset=custom_data, batch_size=32, shuffle=True)\n",
    "\n",
    "custom_data = CustomDatasetFromCSV(valid_set)    \n",
    "valid_loader = torch.utils.data.DataLoader(dataset=custom_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this for both test and validation set\n",
    "def test(dataloader):\n",
    "    dataset_loss = 0\n",
    "    dataset_acc = 0\n",
    "    net.eval()\n",
    "    count = 0    \n",
    "    batches_so_far = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:            \n",
    "            q = x[0]\n",
    "            a = x[1]\n",
    "            if train_on_gpu:\n",
    "                q = q.cuda()\n",
    "                a = a.cuda()\n",
    "                y = y.cuda()                \n",
    "            l = len(y)\n",
    "            if l != batch_size:\n",
    "                # last incomplete batch, skip over it.\n",
    "                continue;\n",
    "            batches_so_far += 1\n",
    "            count += 1\n",
    "            q_hidden, a_hidden = net.init_hidden(batch_size)            \n",
    "            output, q_hidden, a_hidden = net(questions = q, answers = a, q_hidden = q_hidden, a_hidden = a_hidden)\n",
    "            loss = criterion(output, y.float())\n",
    "            interpretation = (output>0.5).float()\n",
    "            equals = interpretation == y.float()           \n",
    "            dataset_loss += loss.item()\n",
    "            dataset_acc += equals.sum()\n",
    "            \n",
    "        dataset_loss = dataset_loss/batches_so_far\n",
    "        dataset_acc = dataset_acc/batches_so_far\n",
    "        \n",
    "    return dataset_loss, dataset_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_this(d1, d2):\n",
    "    lists1 = d1.items() # sorted by key, return a list of tuples\n",
    "    lists2 = d2.items() # sorted by key, return a list of tuples\n",
    "    x1, y1 = zip(*lists1) # unpack a list of pairs into two tuples\n",
    "    x2, y2 = zip(*lists2) # unpack a list of pairs into two tuples\n",
    "    plt.plot(x1, y1, label = \"train\")\n",
    "    plt.plot(x2, y2, label = \"valid\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtl = {}\n",
    "dta = {}   \n",
    "dvl = {}\n",
    "dva = {}\n",
    "\n",
    "lowest_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch=1, print_every=1, clip = 5):\n",
    "    net.train()\n",
    "    train_loss = 0;\n",
    "    train_acc = 0;\n",
    "    valid_loss = 0;\n",
    "    valid_acc = 0;\n",
    "    \n",
    "    \n",
    "    dtl = {}\n",
    "    dta = {}   \n",
    "    dvl = {}\n",
    "    dva = {}\n",
    "\n",
    "    lowest_loss = np.inf\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        count = 0    \n",
    "        batches_so_far = 0\n",
    "        for x, y in train_loader:                \n",
    "            q = x[0]\n",
    "            a = x[1]\n",
    "            if train_on_gpu:\n",
    "                q = q.cuda()\n",
    "                a = a.cuda()\n",
    "                y = y.cuda()                \n",
    "            l = len(y)                        \n",
    "            if l != batch_size:\n",
    "                # last incomplete batch, skip over it.\n",
    "                continue;\n",
    "            batches_so_far +=1\n",
    "            count += l\n",
    "            net.zero_grad()\n",
    "            q_hidden, a_hidden = net.init_hidden(batch_size)            \n",
    "            output, q_hidden, a_hidden = net(questions = q, answers = a, q_hidden = q_hidden, a_hidden = a_hidden)\n",
    "            loss = criterion(output, y.float())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "            interpretation = (output>0.5).float()\n",
    "            equals = interpretation == y.float()            \n",
    "            train_loss += loss.item()\n",
    "            train_acc += equals.sum()\n",
    "\n",
    "            \n",
    "            if count%print_every == 0:\n",
    "                # why counts are batch size and not int\n",
    "                print(\"count \",count)\n",
    "                train_loss = train_loss/batches_so_far\n",
    "                train_acc = train_acc/batches_so_far                \n",
    "                valid_loss, valid_acc = test(valid_loader)\n",
    "                clear_output(wait=True)\n",
    "                print(\"Train Loss \", train_loss, \"\\tTrain Acc \", train_acc, \"\\nValid Loss\", valid_loss, \"\\tValid Acc\", valid_acc)\n",
    "                if  valid_loss < lowest_loss:\n",
    "                    lowest_loss = valid_loss\n",
    "                    model_path = \"../res/mod1.pth\"\n",
    "                    torch.save(net.state_dict(),model_path)                \n",
    "                net.train()                    \n",
    "                dtl[count]= train_loss\n",
    "                dvl[count]= valid_loss\n",
    "                #plot_this(dtl, dvl)\n",
    "                \n",
    "                batches_so_far = 0\n",
    "                train_loss = 0;\n",
    "                train_acc = 0;\n",
    "                valid_loss = 0;\n",
    "                valid_acc = 0;\n",
    "                \n",
    "                if count == 2: break\n",
    "                \n",
    "            #break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss  10.36163330078125 \tTrain Acc  tensor(640, device='cuda:0') \n",
      "Valid Loss 13.762779665357284 \tValid Acc tensor(513, device='cuda:0')\n",
      "count  128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-f5f43323d3d6>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch, print_every, clip)\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatches_so_far\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatches_so_far\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                 \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m                 \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train Loss \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\tTrain Acc \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\nValid Loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\tValid Acc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-deb79401c1fc>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mbatches_so_far\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\WindowsSoftware\\Anaconda\\envs\\acme\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\WindowsSoftware\\Anaconda\\envs\\acme\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-22f617e15042>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0munique_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msupported_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-22f617e15042>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0munique_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msupported_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply embeddings in get data itself\n",
    "# test data loader\n",
    "\n",
    "start_time = time.clock()\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.clock() - start_time))\n",
    "# lstm network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
